{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25890b40-4dd2-4a86-a36e-1cee07b2b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bed6d2-3cb8-4a1c-80c9-a99e0fdbe8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据处理模块\n",
    "def prepare_data(batch_size=128, val_split=0.2, seed=42):\n",
    "    \"\"\"准备MNIST数据集并创建数据加载器\"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    # 创建训练集和验证集\n",
    "    train_size = int((1 - val_split) * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4523828d-d042-494a-846f-344580e8f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 模型构建模块\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=784):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1), :]\n",
    "\n",
    "def create_transformer_model(input_dim=1, d_model=32, nhead=4, num_layers=6, num_classes=10, dropout=0.1):\n",
    "    \"\"\"创建Transformer模型\"\"\"\n",
    "    class TransformerModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(TransformerModel, self).__init__()\n",
    "            \n",
    "            # 嵌入层\n",
    "            self.embedding = nn.Linear(input_dim, d_model)\n",
    "            \n",
    "            # 位置编码\n",
    "            self.pos_encoder = PositionalEncoding(d_model)\n",
    "            \n",
    "            # 变压器编码器层\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model = d_model,\n",
    "                nhead = nhead,\n",
    "                dim_feedforward = 4*d_model,\n",
    "                dropout = dropout,\n",
    "                activation = 'gelu',\n",
    "                batch_first = True\n",
    "            )\n",
    "            \n",
    "            # 变压器编码器\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "            \n",
    "            # 分类头\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, 2*d_model),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(2*d_model, num_classes)\n",
    "            )\n",
    "            \n",
    "            self.d_model = d_model\n",
    "            \n",
    "        def forward(self, x):\n",
    "            batch_size = x.size(0)\n",
    "            x = x.view(batch_size, -1, 1)  # [batch_size, 784, 1]\n",
    "            \n",
    "            # 嵌入和位置编码\n",
    "            x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "            x = self.pos_encoder(x)\n",
    "            \n",
    "            # 通过变压器编码器\n",
    "            x = self.transformer_encoder(x)\n",
    "            \n",
    "            # 全局平均池化\n",
    "            x = x.mean(dim=1)\n",
    "            \n",
    "            # 分类\n",
    "            logits = self.classifier(x)\n",
    "            \n",
    "            return logits\n",
    "    \n",
    "    return TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1f7040-0ea5-4f4e-ad49-99f437c1c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 训练模块\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"验证模型\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, model_path='best_model.pth'):\n",
    "    \"\"\"训练完整模型\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 获取当前学习率\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f'Epoch {epoch}: Current learning rate is {current_lr}')\n",
    "        \n",
    "        print(f'Epoch: {epoch}/{epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print('Model saved!\\n')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b7a177-2107-4ff4-ae75-e38cbd7336b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 评估模块\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, preds = output.max(1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    report = classification_report(all_targets, all_preds)\n",
    "    \n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23029ca0-be7c-45fa-ba80-22456c37cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 可视化模块\n",
    "def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    \"\"\"绘制训练曲线\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83202d56-a474-4ea0-a1f6-bc241e3d0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 样本生成模块\n",
    "def generate_samples(model, test_loader, num_samples=5000, device='cpu', save_path='generated_samples'):\n",
    "    \"\"\"生成样本并保存\"\"\"\n",
    "    model.eval()\n",
    "    generated_samples = []\n",
    "    generated_labels = []\n",
    "    \n",
    "    # 从测试集获取样本\n",
    "    all_data = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            all_data.append(data)\n",
    "            all_targets.append(target)\n",
    "    \n",
    "    all_data = torch.cat(all_data, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    # 随机选择样本\n",
    "    indices = torch.randperm(len(all_data))[:num_samples]\n",
    "    samples = all_data[indices].to(device)\n",
    "    true_labels = all_targets[indices].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(samples)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        generated_samples = samples.cpu().numpy()\n",
    "        generated_labels = predicted.cpu().numpy()\n",
    "        true_labels = true_labels.cpu().numpy()\n",
    "    \n",
    "    # 保存生成的样本\n",
    "    np.save(f'{save_path}_images.npy', generated_samples)\n",
    "    np.save(f'{save_path}_predicted_labels.npy', generated_labels)\n",
    "    np.save(f'{save_path}_true_labels.npy', true_labels)\n",
    "    \n",
    "    return generated_samples, generated_labels, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4acb3f7c-d921-44f7-bea1-7376914c043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "def main(epochs):\n",
    "    \n",
    "    # 设置随机种子\n",
    "    set_seed(42)\n",
    "    \n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # 准备数据\n",
    "    train_loader, val_loader, test_loader = prepare_data(batch_size=32)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = create_transformer_model().to(device)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    # 定义学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # 训练模型\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, model_path='best_transformer_mnist_model.pth'\n",
    "    )\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load('best_transformer_mnist_model.pth'))\n",
    "    \n",
    "    # 评估模型\n",
    "    test_accuracy, test_report = evaluate_model(model, test_loader, device)\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    print(f'Classification Report:')\n",
    "    print(test_report)\n",
    "\n",
    "    \n",
    "    # 生成样本\n",
    "    generate_samples(model, test_loader, num_samples=5000, device=device)\n",
    "    print(\"Training and evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e30c91-fd98-4102-bffd-cec9a0e86534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\n",
      "Train Loss: 1.9939 | Train Acc: 25.04%\n",
      "Val Loss: 1.8013 | Val Acc: 30.63%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 2/50\n",
      "Train Loss: 1.7121 | Train Acc: 33.49%\n",
      "Val Loss: 1.5409 | Val Acc: 41.43%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 3/50\n",
      "Train Loss: 1.4744 | Train Acc: 43.06%\n",
      "Val Loss: 1.3633 | Val Acc: 48.00%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 4/50\n",
      "Train Loss: 1.3295 | Train Acc: 50.04%\n",
      "Val Loss: 1.3123 | Val Acc: 52.24%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 5/50\n",
      "Train Loss: 1.2132 | Train Acc: 54.92%\n",
      "Val Loss: 1.2615 | Val Acc: 54.26%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 6/50\n",
      "Train Loss: 1.0876 | Train Acc: 60.84%\n",
      "Val Loss: 1.5350 | Val Acc: 47.88%\n",
      "Epoch: 7/50\n",
      "Train Loss: 0.9947 | Train Acc: 64.24%\n",
      "Val Loss: 1.4455 | Val Acc: 51.42%\n",
      "Epoch: 8/50\n",
      "Train Loss: 0.9236 | Train Acc: 66.69%\n",
      "Val Loss: 1.3244 | Val Acc: 54.26%\n",
      "Epoch: 9/50\n",
      "Train Loss: 0.8808 | Train Acc: 68.13%\n",
      "Val Loss: 0.9467 | Val Acc: 65.76%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 10/50\n",
      "Train Loss: 0.8286 | Train Acc: 70.22%\n",
      "Val Loss: 0.8375 | Val Acc: 70.04%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 11/50\n",
      "Train Loss: 0.7841 | Train Acc: 71.74%\n",
      "Val Loss: 0.8381 | Val Acc: 69.58%\n",
      "Epoch: 12/50\n",
      "Train Loss: 0.7558 | Train Acc: 72.64%\n",
      "Val Loss: 0.8636 | Val Acc: 70.65%\n",
      "Epoch: 13/50\n",
      "Train Loss: 0.7157 | Train Acc: 74.65%\n",
      "Val Loss: 0.8465 | Val Acc: 71.03%\n",
      "Epoch: 14/50\n",
      "Train Loss: 0.6866 | Train Acc: 75.60%\n",
      "Val Loss: 0.6712 | Val Acc: 76.82%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 15/50\n",
      "Train Loss: 0.6592 | Train Acc: 76.82%\n",
      "Val Loss: 0.7537 | Val Acc: 73.43%\n",
      "Epoch: 16/50\n",
      "Train Loss: 0.6403 | Train Acc: 77.73%\n",
      "Val Loss: 0.6300 | Val Acc: 78.72%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 17/50\n",
      "Train Loss: 0.6174 | Train Acc: 78.71%\n",
      "Val Loss: 0.5576 | Val Acc: 81.18%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 18/50\n",
      "Train Loss: 0.6002 | Train Acc: 79.33%\n",
      "Val Loss: 0.5824 | Val Acc: 80.64%\n",
      "Epoch: 19/50\n",
      "Train Loss: 0.5809 | Train Acc: 80.21%\n",
      "Val Loss: 0.5049 | Val Acc: 83.11%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 20/50\n",
      "Train Loss: 0.5639 | Train Acc: 80.99%\n",
      "Val Loss: 0.5573 | Val Acc: 81.93%\n",
      "Epoch: 21/50\n",
      "Train Loss: 0.5543 | Train Acc: 81.29%\n",
      "Val Loss: 0.5218 | Val Acc: 82.85%\n",
      "Epoch: 22/50\n",
      "Train Loss: 0.5290 | Train Acc: 82.24%\n",
      "Val Loss: 0.5270 | Val Acc: 82.81%\n",
      "Epoch: 23/50\n",
      "Train Loss: 0.5168 | Train Acc: 82.79%\n",
      "Val Loss: 0.4707 | Val Acc: 84.81%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 24/50\n",
      "Train Loss: 0.5036 | Train Acc: 83.41%\n",
      "Val Loss: 0.4381 | Val Acc: 86.11%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 25/50\n",
      "Train Loss: 0.4788 | Train Acc: 84.10%\n",
      "Val Loss: 0.4350 | Val Acc: 85.88%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 26/50\n",
      "Train Loss: 0.4712 | Train Acc: 84.39%\n",
      "Val Loss: 0.4037 | Val Acc: 87.04%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 27/50\n",
      "Train Loss: 0.4525 | Train Acc: 84.96%\n",
      "Val Loss: 0.4371 | Val Acc: 85.25%\n",
      "Epoch: 28/50\n",
      "Train Loss: 0.4397 | Train Acc: 85.80%\n",
      "Val Loss: 0.4507 | Val Acc: 85.61%\n",
      "Epoch: 29/50\n",
      "Train Loss: 0.4241 | Train Acc: 86.21%\n",
      "Val Loss: 0.3736 | Val Acc: 88.17%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 30/50\n",
      "Train Loss: 0.4086 | Train Acc: 86.77%\n",
      "Val Loss: 0.4149 | Val Acc: 86.72%\n",
      "Epoch: 31/50\n",
      "Train Loss: 0.3959 | Train Acc: 87.21%\n",
      "Val Loss: 0.3758 | Val Acc: 88.26%\n",
      "Epoch: 32/50\n",
      "Train Loss: 0.3881 | Train Acc: 87.43%\n",
      "Val Loss: 0.4572 | Val Acc: 85.88%\n",
      "Epoch: 33/50\n",
      "Train Loss: 0.3770 | Train Acc: 87.68%\n",
      "Val Loss: 0.4342 | Val Acc: 86.52%\n",
      "Epoch: 34/50\n",
      "Train Loss: 0.3360 | Train Acc: 89.13%\n",
      "Val Loss: 0.3166 | Val Acc: 89.92%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 35/50\n",
      "Train Loss: 0.3316 | Train Acc: 89.39%\n",
      "Val Loss: 0.3266 | Val Acc: 89.39%\n",
      "Epoch: 36/50\n",
      "Train Loss: 0.3245 | Train Acc: 89.59%\n",
      "Val Loss: 0.3236 | Val Acc: 89.62%\n",
      "Epoch: 37/50\n",
      "Train Loss: 0.3189 | Train Acc: 89.90%\n",
      "Val Loss: 0.3193 | Val Acc: 89.83%\n",
      "Epoch: 38/50\n",
      "Train Loss: 0.3147 | Train Acc: 89.85%\n",
      "Val Loss: 0.3071 | Val Acc: 90.11%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 39/50\n",
      "Train Loss: 0.3094 | Train Acc: 90.00%\n",
      "Val Loss: 0.2986 | Val Acc: 90.37%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 40/50\n",
      "Train Loss: 0.3025 | Train Acc: 90.41%\n",
      "Val Loss: 0.2989 | Val Acc: 90.29%\n",
      "Epoch: 41/50\n",
      "Train Loss: 0.3014 | Train Acc: 90.34%\n",
      "Val Loss: 0.2955 | Val Acc: 90.56%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 42/50\n",
      "Train Loss: 0.3021 | Train Acc: 90.21%\n",
      "Val Loss: 0.3012 | Val Acc: 90.50%\n",
      "Epoch: 43/50\n",
      "Train Loss: 0.2933 | Train Acc: 90.61%\n",
      "Val Loss: 0.3138 | Val Acc: 90.08%\n",
      "Epoch: 44/50\n",
      "Train Loss: 0.2891 | Train Acc: 90.72%\n",
      "Val Loss: 0.2841 | Val Acc: 91.00%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 45/50\n",
      "Train Loss: 0.2863 | Train Acc: 90.91%\n",
      "Val Loss: 0.2865 | Val Acc: 90.97%\n",
      "Epoch: 46/50\n",
      "Train Loss: 0.2830 | Train Acc: 90.85%\n",
      "Val Loss: 0.3062 | Val Acc: 90.28%\n",
      "Epoch: 47/50\n",
      "Train Loss: 0.2778 | Train Acc: 91.07%\n",
      "Val Loss: 0.2718 | Val Acc: 91.41%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 48/50\n",
      "Train Loss: 0.2754 | Train Acc: 91.09%\n",
      "Val Loss: 0.2620 | Val Acc: 91.69%\n",
      "Model saved!\n",
      "\n",
      "Epoch: 49/50\n",
      "Train Loss: 0.2721 | Train Acc: 91.22%\n",
      "Val Loss: 0.2886 | Val Acc: 90.90%\n",
      "Epoch: 50/50\n",
      "Train Loss: 0.2699 | Train Acc: 91.31%\n",
      "Val Loss: 0.2639 | Val Acc: 91.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3072/2239485089.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_transformer_mnist_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9265\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       980\n",
      "           1       0.99      0.97      0.98      1135\n",
      "           2       0.92      0.93      0.93      1032\n",
      "           3       0.91      0.87      0.89      1010\n",
      "           4       0.92      0.93      0.93       982\n",
      "           5       0.87      0.87      0.87       892\n",
      "           6       0.93      0.94      0.94       958\n",
      "           7       0.96      0.92      0.94      1028\n",
      "           8       0.91      0.94      0.93       974\n",
      "           9       0.88      0.91      0.89      1009\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "Training and evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
