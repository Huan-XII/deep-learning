{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365adfc5-7b0b-4846-8da1-72a07e11b0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# 全局设置中文字体，以“SimSun（宋体）”为例，也可用“Microsoft YaHei（微软雅黑）”等\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4087c4-aa4e-4285-8f6a-2d7043c1caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers_per_block, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers_per_block = num_layers_per_block\n",
    "        \n",
    "        # 定义6层LSTM网络结构\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers_per_block, batch_first=True, dropout=dropout)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers_per_block, batch_first=True, dropout=dropout)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, num_layers_per_block, batch_first=True, dropout=dropout)\n",
    "        self.lstm4 = nn.LSTM(hidden_size, hidden_size, num_layers_per_block, batch_first=True, dropout=dropout)\n",
    "        self.lstm5 = nn.LSTM(hidden_size, hidden_size, num_layers_per_block, batch_first=True, dropout=dropout)\n",
    "        self.lstm6 = nn.LSTM(hidden_size, hidden_size, num_layers_per_block, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()  # 隐藏层使用ReLU激活函数\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"初始化LSTM和全连接层权重\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lstm' in name and 'weight' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'fc' in name and 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 为每层LSTM初始化隐藏状态\n",
    "        h0_1 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_1 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        h0_2 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_2 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        h0_3 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_3 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        h0_4 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_4 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        h0_5 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_5 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        h0_6 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0_6 = torch.zeros(self.num_layers_per_block, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # 前向传播LSTM\n",
    "        out, _ = self.lstm1(x, (h0_1, c0_1))\n",
    "        out = self.relu(out)\n",
    "        out, _ = self.lstm2(out, (h0_2, c0_2))\n",
    "        out = self.relu(out)\n",
    "        out, _ = self.lstm3(out, (h0_3, c0_3))\n",
    "        out = self.relu(out)\n",
    "        out, _ = self.lstm4(out, (h0_4, c0_4))\n",
    "        out = self.relu(out)\n",
    "        out, _ = self.lstm5(out, (h0_5, c0_5))\n",
    "        out = self.relu(out)\n",
    "        out, _ = self.lstm6(out, (h0_6, c0_6))\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # 只取序列的最后一个时间步的输出\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4220df08-98f3-4daf-95f9-df0b16589208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uci_data(url):\n",
    "    \"\"\"下载UCI数据集\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            raise Exception(f\"下载失败，状态码: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"下载数据时出错: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b2429c-9081-4b18-bc70-77b05cec4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, seq_length, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"数据预处理和序列生成\"\"\"\n",
    "    # 假设数据最后一列为目标变量\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    # 数据标准化\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 生成序列\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    \n",
    "    X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    # 划分训练集、验证集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=test_size, random_state=42, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size/(1-test_size), random_state=42, shuffle=False)\n",
    "    \n",
    "    # 转换为PyTorch张量\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.FloatTensor(y_val)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab4fd13-93f8-4d71-9db7-ce226a36ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device, scheduler=None, gradient_clip=None, early_stopping_patience=10):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.unsqueeze(1))\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            if gradient_clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # 记录损失\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 学习率调度\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stopping_patience:\n",
    "                print(f\"早停触发：验证集损失 {early_stopping_patience} 个epoch未改善\")\n",
    "                break\n",
    "                \n",
    "        # 打印训练进度\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}')\n",
    "    \n",
    "\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9270591-df31-4fc7-b415-a306b1c3c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, X_test, scaler_y, device, num_samples=5000):\n",
    "    \"\"\"生成样本并反标准化\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test = X_test.to(device)\n",
    "        # 从测试数据中随机选择起点生成样本\n",
    "        indices = np.random.randint(0, len(X_test), num_samples)\n",
    "        samples = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            sample = model(X_test[idx:idx+1])\n",
    "            samples.append(sample.cpu().numpy())\n",
    "    \n",
    "    samples = np.array(samples).reshape(-1, 1)\n",
    "    # 反标准化\n",
    "    samples = scaler_y.inverse_transform(samples)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8109e1-e097-4d84-afcd-dd0c61dcd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, scaler_y, device):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        predictions = model(X_test)\n",
    "        \n",
    "        # 反标准化\n",
    "        predictions = scaler_y.inverse_transform(predictions.cpu().numpy())\n",
    "        y_test = scaler_y.inverse_transform(y_test.cpu().numpy().reshape(-1, 1))\n",
    "    \n",
    "    # 计算评估指标\n",
    "    mse = np.mean((predictions - y_test) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(predictions - y_test))\n",
    "    mape = np.mean(np.abs((predictions - y_test) / y_test)) * 100\n",
    "    \n",
    "    print(f'评估指标:')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'MAPE: {mape:.2f}%')\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c712da-5e28-4564-bf46-d31f34e07ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"绘制训练和验证损失曲线\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Loss of train set')\n",
    "    plt.plot(val_losses, label='Loss of Validation set')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss curve of Train and Validation set')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d04be4d0-3212-4879-8ff3-9983d40ffae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(epochs, lr):\n",
    "#     # 数据集URL（以空气质量数据集为例）\n",
    "#     data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.csv'\n",
    "    \n",
    "#     # 下载数据\n",
    "#     print(\"正在下载数据集...\")\n",
    "#     data_text = download_uci_data(data_url)\n",
    "#     if data_text is None:\n",
    "#         print(\"无法下载数据，尝试使用本地数据或其他数据集\")\n",
    "#         return\n",
    "\n",
    "    # 解析数据\n",
    "    print(\"正在解析数据...\")\n",
    "    # 注意：实际应用中可能需要根据特定数据集调整解析方式\n",
    "    data = pd.read_csv('./data/AirQuality/AirQualityUCI.csv', sep=';', decimal=',')\n",
    "\n",
    "    # 删除指定的两列（axis=1 表示列）\n",
    "    data = data.drop(['Unnamed: 15', 'Unnamed: 16'], axis=1)\n",
    "\n",
    "    # 移除包含NaN的行\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # 移除文本列（日期和时间）\n",
    "    data = data.iloc[:, 2:].values\n",
    "    \n",
    "    # 数据预处理\n",
    "    seq_length = 48  # 使用前48个时间步预测下一个时间步\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, scaler_y = preprocess_data(data, seq_length)\n",
    "    \n",
    "    print(f\"训练集形状: {X_train.shape},  验证集形状: {X_val.shape},  测试集形状: {X_test.shape}\")\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    batch_size = 32\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    input_size = X_train.shape[2]  # 特征数量\n",
    "    hidden_size = 128\n",
    "    num_layers_per_block = 3  # 每层LSTM的层数\n",
    "    output_size = 1  # 预测值数量\n",
    "    dropout = 0.2  # 添加dropout防止过拟合\n",
    "    \n",
    "    model = LSTMModel(input_size, hidden_size, num_layers_per_block, output_size, dropout).to(device)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr)  # Adam优化器\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5,  # 学习率降低因子\n",
    "        patience=5,  # 等待5个epoch没有改善再降低学习率\n",
    "        verbose=True  # 打印学习率调整信息\n",
    "    )\n",
    "    \n",
    "    # 训练参数\n",
    "    # epochs = 200  # 增加最大训练轮次\n",
    "    gradient_clip = 1.0  # 梯度裁剪阈值\n",
    "    early_stopping_patience = 50  # 早停等待轮次\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"开始训练模型...\")\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, epochs, device,\n",
    "        scheduler=scheduler, gradient_clip=gradient_clip, early_stopping_patience=early_stopping_patience\n",
    "    )\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plot_losses(train_losses, val_losses)\n",
    "    \n",
    "    # 生成样本\n",
    "    print(\"正在生成样本...\")\n",
    "    samples = generate_samples(model, X_test, scaler_y, device, num_samples=5000)\n",
    "    \n",
    "    # 保存样本\n",
    "    np.savetxt('generated_samples.csv', samples, delimiter=',')\n",
    "    print(\"样本已保存至 generated_samples.csv\")\n",
    "    \n",
    "    # 评估模型\n",
    "    print(\"正在评估模型...\")\n",
    "    metrics = evaluate_model(model, X_test, y_test, scaler_y, device)\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "    print(\"模型已保存至 lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b9453d-ef45-4789-baf9-e3b8251be3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在解析数据...\n",
      "训练集形状: torch.Size([5585, 48, 12]),  验证集形状: torch.Size([1862, 48, 12]),  测试集形状: torch.Size([1862, 48, 12])\n",
      "使用设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n",
      "Epoch [10/1000], Train Loss: 0.6901, Val Loss: 1.7870, LR: 0.000250\n",
      "Epoch [20/1000], Train Loss: 0.6862, Val Loss: 1.7835, LR: 0.000250\n",
      "Epoch [30/1000], Train Loss: 0.6899, Val Loss: 1.7819, LR: 0.000063\n",
      "Epoch [40/1000], Train Loss: 0.6899, Val Loss: 1.7816, LR: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial Unicode MS\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial Unicode MS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早停触发：验证集损失 30 个epoch未改善\n",
      "正在生成样本...\n",
      "样本已保存至 generated_samples.csv\n",
      "正在评估模型...\n",
      "评估指标:\n",
      "MSE: 1794.0538\n",
      "RMSE: 42.3563\n",
      "MAE: 13.5381\n",
      "MAPE: 834.64%\n",
      "模型已保存至 lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(epochs = 1000, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d6d28-bec3-4ffb-a19a-1716839933d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29701aa-8844-4adc-b3e4-82c9550213f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e697525-ad56-483b-9335-9f56c403f222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
