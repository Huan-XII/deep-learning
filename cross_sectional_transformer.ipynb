{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46283e96-1d62-403e-854d-b01870d5c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132d364b-0978-44a5-b370-a3359e06e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 虚构数据集生成器\n",
    "def generate_synthetic_data(num_samples=2666, num_features=26):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(num_samples, num_features) * 2 + 1\n",
    "    # 创建非线性目标函数\n",
    "    y = (X[:, 0] ** 2 + np.sin(X[:, 1] * 2) + \n",
    "         X[:, 5] * X[:, 7] + 0.5 * np.random.randn(num_samples))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0a5889-9462-42b8-90d4-880f029f2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载器\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.X = torch.FloatTensor(features)\n",
    "        self.y = torch.FloatTensor(targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d629a5c0-61ef-4477-9be4-69b5b98d6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码模块\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=26):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6665be9-ff6e-4b41-829f-9fc7814ea375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer回归模型\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=26, d_model=128, nhead=4, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.feature_embedding = nn.Linear(1, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward=4*d_model, dropout=0.1, batch_first=True)\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.feature_embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.regressor(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eec0dec-bd4b-4122-b3f7-72b44e1dee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, scheduler, epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(loader))\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1} | Loss: {loss.item():.2f}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc275f4-c6e5-4a24-b889-21f9fb5dfc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练损失可视化\n",
    "def plot_loss_curve(train_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c70808-e6e6-41f7-8fa8-2ce7326c214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测结果对比可视化\n",
    "def plot_predictions(strs, y_true, y_pred, sample_count=50):\n",
    "\n",
    "    mse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.scatter(y_true[:sample_count], y_pred[:sample_count],\n",
    "                color='blue', label=str(strs) + ' (n='+str(len(y_true))+')', alpha=0.6)\n",
    "    \n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], linewidth=2.5 , c=\"orange\")\n",
    "    \n",
    "    plt.text(min(y_true)-0.2, max(max(y_true),max(y_pred))-2, '$R^{2}$ on Traing set='+str(round(r2,2)), fontsize=12)\n",
    "    plt.text(min(y_true)-0.2, max(max(y_true),max(y_pred))-6, '$RMSE$ on Traing set='+str(round(mse,2)), fontsize=12)\n",
    "\n",
    "    plt.xlabel('True values')\n",
    "    plt.ylabel('Prediction values')\n",
    "    plt.title('True VS Predicted Values')\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(str(strs) + ' predictions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb28e891-b57f-436f-a6da-edc4ec1dbee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 35.10\n",
      "Epoch 2 | Loss: 65.78\n",
      "Epoch 3 | Loss: 109.49\n",
      "Epoch 4 | Loss: 58.54\n",
      "Epoch 5 | Loss: 51.52\n",
      "Epoch 6 | Loss: 33.90\n",
      "Epoch 7 | Loss: 17.53\n",
      "Epoch 8 | Loss: 41.85\n",
      "Epoch 9 | Loss: 16.95\n",
      "Epoch 10 | Loss: 25.85\n",
      "Epoch 11 | Loss: 11.09\n",
      "Epoch 12 | Loss: 21.08\n",
      "Epoch 13 | Loss: 13.38\n",
      "Epoch 14 | Loss: 24.30\n",
      "Epoch 15 | Loss: 20.92\n",
      "Epoch 16 | Loss: 5.34\n",
      "Epoch 17 | Loss: 4.86\n",
      "Epoch 18 | Loss: 23.46\n",
      "Epoch 19 | Loss: 10.36\n",
      "Epoch 20 | Loss: 36.50\n",
      "Epoch 21 | Loss: 18.45\n",
      "Epoch 22 | Loss: 7.38\n",
      "Epoch 23 | Loss: 11.24\n",
      "Epoch 24 | Loss: 8.40\n",
      "Epoch 25 | Loss: 3.98\n",
      "Epoch 26 | Loss: 11.57\n",
      "Epoch 27 | Loss: 6.95\n",
      "Epoch 28 | Loss: 9.81\n",
      "Epoch 29 | Loss: 5.84\n",
      "Epoch 30 | Loss: 9.34\n",
      "Epoch 31 | Loss: 10.17\n",
      "Epoch 32 | Loss: 9.39\n",
      "Epoch 33 | Loss: 6.83\n",
      "Epoch 34 | Loss: 7.82\n",
      "Epoch 35 | Loss: 3.14\n",
      "Epoch 36 | Loss: 6.17\n",
      "Epoch 37 | Loss: 7.33\n",
      "Epoch 38 | Loss: 8.19\n",
      "Epoch 39 | Loss: 5.34\n",
      "Epoch 40 | Loss: 5.14\n",
      "Epoch 41 | Loss: 5.14\n",
      "Epoch 42 | Loss: 3.95\n",
      "Epoch 43 | Loss: 2.54\n",
      "Epoch 44 | Loss: 7.71\n",
      "Epoch 45 | Loss: 3.61\n",
      "Epoch 46 | Loss: 6.88\n",
      "Epoch 47 | Loss: 3.85\n",
      "Epoch 48 | Loss: 6.85\n",
      "Epoch 49 | Loss: 7.49\n",
      "Epoch 50 | Loss: 2.29\n",
      "Epoch 51 | Loss: 2.92\n",
      "Epoch 52 | Loss: 4.33\n",
      "Epoch 53 | Loss: 4.33\n",
      "Epoch 54 | Loss: 1.84\n",
      "Epoch 55 | Loss: 9.46\n",
      "Epoch 56 | Loss: 2.54\n",
      "Epoch 57 | Loss: 8.72\n",
      "Epoch 58 | Loss: 3.58\n",
      "Epoch 59 | Loss: 2.51\n",
      "Epoch 60 | Loss: 9.43\n",
      "Epoch 61 | Loss: 5.69\n",
      "Epoch 62 | Loss: 1.49\n",
      "Epoch 63 | Loss: 10.09\n",
      "Epoch 64 | Loss: 2.60\n",
      "Epoch 65 | Loss: 4.45\n",
      "Epoch 66 | Loss: 5.21\n",
      "Epoch 67 | Loss: 1.53\n",
      "Epoch 68 | Loss: 1.44\n",
      "Epoch 69 | Loss: 3.13\n",
      "Epoch 70 | Loss: 3.52\n",
      "Epoch 71 | Loss: 2.88\n",
      "Epoch 72 | Loss: 2.24\n",
      "Epoch 73 | Loss: 3.34\n",
      "Epoch 74 | Loss: 2.29\n",
      "Epoch 75 | Loss: 1.17\n",
      "Epoch 76 | Loss: 1.82\n",
      "Epoch 77 | Loss: 3.59\n",
      "Epoch 78 | Loss: 3.00\n",
      "Epoch 79 | Loss: 3.32\n",
      "Epoch 80 | Loss: 3.38\n",
      "Epoch 81 | Loss: 3.57\n",
      "Epoch 82 | Loss: 7.04\n",
      "Epoch 83 | Loss: 1.15\n",
      "Epoch 84 | Loss: 1.26\n",
      "Epoch 85 | Loss: 2.88\n",
      "Epoch 86 | Loss: 0.88\n",
      "Epoch 87 | Loss: 0.51\n",
      "Epoch 88 | Loss: 1.42\n",
      "Epoch 89 | Loss: 2.36\n",
      "Epoch 90 | Loss: 23.85\n",
      "Epoch 91 | Loss: 3.06\n",
      "Epoch 92 | Loss: 1.61\n",
      "Epoch 93 | Loss: 2.31\n",
      "Epoch 94 | Loss: 5.61\n",
      "Epoch 95 | Loss: 2.41\n",
      "Epoch 96 | Loss: 2.16\n",
      "Epoch 97 | Loss: 0.87\n",
      "Epoch 98 | Loss: 1.44\n",
      "Epoch 99 | Loss: 2.49\n",
      "Epoch 100 | Loss: 2.23\n",
      "Epoch 101 | Loss: 3.01\n",
      "Epoch 102 | Loss: 1.29\n",
      "Epoch 103 | Loss: 2.01\n",
      "Epoch 104 | Loss: 20.15\n",
      "Epoch 105 | Loss: 1.73\n",
      "Epoch 106 | Loss: 1.80\n",
      "Epoch 107 | Loss: 2.15\n",
      "Epoch 108 | Loss: 1.73\n",
      "Epoch 109 | Loss: 3.85\n",
      "Epoch 110 | Loss: 5.29\n",
      "Epoch 111 | Loss: 0.85\n",
      "Epoch 112 | Loss: 11.52\n",
      "Epoch 113 | Loss: 2.66\n",
      "Epoch 114 | Loss: 1.81\n",
      "Epoch 115 | Loss: 4.13\n",
      "Epoch 116 | Loss: 15.50\n",
      "Epoch 117 | Loss: 1.72\n",
      "Epoch 118 | Loss: 1.30\n",
      "Epoch 119 | Loss: 3.11\n",
      "Epoch 120 | Loss: 1.96\n",
      "Epoch 121 | Loss: 2.09\n",
      "Epoch 122 | Loss: 1.11\n",
      "Epoch 123 | Loss: 2.59\n",
      "Epoch 124 | Loss: 1.70\n",
      "Epoch 125 | Loss: 2.72\n",
      "Epoch 126 | Loss: 1.32\n",
      "Epoch 127 | Loss: 2.54\n",
      "Epoch 128 | Loss: 2.51\n",
      "Epoch 129 | Loss: 1.45\n",
      "Epoch 130 | Loss: 1.37\n",
      "Epoch 131 | Loss: 2.44\n",
      "Epoch 132 | Loss: 1.77\n",
      "Epoch 133 | Loss: 1.72\n",
      "Epoch 134 | Loss: 22.57\n",
      "Epoch 135 | Loss: 1.30\n",
      "Epoch 136 | Loss: 2.03\n",
      "Epoch 137 | Loss: 2.62\n",
      "Epoch 138 | Loss: 2.92\n",
      "Epoch 139 | Loss: 2.42\n",
      "Epoch 140 | Loss: 1.19\n",
      "Epoch 141 | Loss: 1.73\n",
      "Epoch 142 | Loss: 1.24\n",
      "Epoch 143 | Loss: 1.67\n",
      "Epoch 144 | Loss: 2.41\n",
      "Epoch 145 | Loss: 5.63\n",
      "Epoch 146 | Loss: 2.40\n",
      "Epoch 147 | Loss: 1.20\n",
      "Epoch 148 | Loss: 1.61\n",
      "Epoch 149 | Loss: 2.48\n",
      "Epoch 150 | Loss: 3.54\n",
      "Test MSE: 2.8289\n",
      "Train MSE: 1.8197\n"
     ]
    }
   ],
   "source": [
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成并划分数据\n",
    "    X, y = generate_synthetic_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 数据标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_dataset = FeatureDataset(X_train, y_train)\n",
    "    test_dataset = FeatureDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = TransformerRegressor()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    \n",
    "    # 训练模型\n",
    "    train_losses = train(model, train_loader, criterion, optimizer, scheduler, epochs=150)\n",
    "    \n",
    "    # 可视化训练过程\n",
    "    plot_loss_curve(train_losses)\n",
    "\n",
    "    \n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(torch.FloatTensor(X_test))\n",
    "        test_loss = criterion(test_preds, torch.FloatTensor(y_test))\n",
    "        print(f'Test MSE: {test_loss.item():.4f}')\n",
    "        plot_predictions('Test set', y_test, test_preds)\n",
    "        \n",
    "        train_preds = model(torch.FloatTensor(X_train))\n",
    "        train_loss = criterion(train_preds, torch.FloatTensor(y_train))\n",
    "        print(f'Train MSE: {train_loss.item():.4f}')\n",
    "        plot_predictions('Train set', y_train, train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddce2b-4b9e-4a15-8d37-d6827292dd20",
   "metadata": {},
   "source": [
    "####  排列重要性分析（最推荐）\n",
    "通过随机打乱特征值观察模型性能变化，适用于任何模型类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fecbd-a644-4ad9-85b2-b4ca74fe282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_permutation_importance(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    # 转换为numpy数组\n",
    "    X_test_np = X_test.numpy()\n",
    "    y_test_np = y_test.numpy()\n",
    "    \n",
    "    # 计算基准得分\n",
    "    with torch.no_grad():\n",
    "        baseline_score = -criterion(model(X_test), y_test).item()\n",
    "\n",
    "    # 计算排列重要性\n",
    "    result = permutation_importance(\n",
    "        estimator=model,\n",
    "        X=X_test_np,\n",
    "        y=y_test_np,\n",
    "        n_repeats=10,\n",
    "        scoring=lambda model, X, y: -criterion(\n",
    "            model(torch.FloatTensor(X)), \n",
    "            torch.FloatTensor(y)\n",
    "        ).item(),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return result.importances_mean, baseline_score\n",
    "\n",
    "# 主程序添加\n",
    "if __name__ == \"__main__\":\n",
    "    # ...（训练代码后添加）\n",
    "    # 计算特征重要性\n",
    "    importances, baseline = calculate_permutation_importance(model, test_dataset.X, test_dataset.y)\n",
    "    \n",
    "    # 可视化\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.title(\"Feature Importance via Permutation\")\n",
    "    plt.bar(range(X_train.shape‌:ml-citation{ref=\"1\" data=\"citationList\"}), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape‌:ml-citation{ref=\"1\" data=\"citationList\"}), indices)\n",
    "    plt.xlim([-1, X_train.shape‌:ml-citation{ref=\"1\" data=\"citationList\"}])\n",
    "    plt.ylabel(\"Mean Accuracy Decrease\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed436c-1a8d-4e1e-9175-fae33947cc02",
   "metadata": {},
   "source": [
    "#### 注意力权重分析（Transformer特有）\n",
    "提取Transformer注意力层的权重分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea06c0-1f9e-4dfc-b4ee-d4fda3e82389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableTransformer(TransformerRegressor):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.feature_embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        attentions = []\n",
    "        for layer in self.transformer.layers:\n",
    "            x, attn = layer.self_attn(x, x, x, need_weights=True)\n",
    "            attentions.append(attn.detach().cpu())\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.regressor(x).squeeze(-1), attentions\n",
    "\n",
    "# 分析函数\n",
    "def visualize_attention(attentions):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, attn in enumerate(attentions):\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        plt.imshow(attn.mean(0), cmap='viridis')\n",
    "        plt.title(f\"Layer {i+1} Attention\")\n",
    "        plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b90662-0d1a-4dbc-8635-21694c906c09",
   "metadata": {},
   "source": [
    "#### SHAP值分析（个体特征解释）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89566ef-424a-471c-80bc-57e95e7a92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def shap_analysis(model, sample_data):\n",
    "    # 创建解释器\n",
    "    explainer = shap.DeepExplainer(\n",
    "        model,\n",
    "        train_loader.dataset.X[:100]  # 背景数据集\n",
    "    )\n",
    "    \n",
    "    # 计算SHAP值\n",
    "    shap_values = explainer.shap_values(sample_data)\n",
    "    \n",
    "    # 可视化\n",
    "    shap.summary_plot(shap_values, sample_data, feature_names=[f\"F{i}\" for i in range(26)])\n",
    "    shap.plots.bar(shap_values, max_display=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cfe54d-2ff4-4bd1-926f-fa44be6c37ef",
   "metadata": {},
   "source": [
    "#### 梯度重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf649383-3372-4f22-86c4-175aa030b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_feature_importance(model, sample_X):\n",
    "    model.eval()\n",
    "    sample_X.requires_grad = True\n",
    "    \n",
    "    output = model(sample_X.unsqueeze(0))\n",
    "    output.backward()\n",
    "    \n",
    "    gradients = torch.abs(sample_X.grad)\n",
    "    normalized = gradients / gradients.sum()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(normalized)), normalized.detach().numpy())\n",
    "    plt.title(\"Gradient-based Feature Importance\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Normalized Gradient Magnitude\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e36d2-deda-4cda-92eb-159441605aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ...（原有训练代码）\n",
    "    \n",
    "    # === 特征重要性分析 ===\n",
    "    # 方法1: 排列重要性\n",
    "    importances, _ = calculate_permutation_importance(model, test_dataset.X, test_dataset.y)\n",
    "    \n",
    "    # 方法2: 注意力可视化\n",
    "    interpret_model = InterpretableTransformer()\n",
    "    _, attentions = interpret_model(test_dataset.X[:1])\n",
    "    visualize_attention(attentions)\n",
    "    \n",
    "    # 方法3: SHAP分析\n",
    "    shap_analysis(model, test_dataset.X[:10])\n",
    "    \n",
    "    # 方法4: 梯度重要性\n",
    "    sample_idx = 0\n",
    "    gradient_feature_importance(model, test_dataset.X[sample_idx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
